# knowledge-distillation

Where:
- `CE` is Cross Entropy
- `KLDiv` is Kullbackâ€“Leibler divergence between soft predictions
- `T` is temperature to soften logits

---

## ðŸ“Š Distillation Variants Included

| Type                           | Description                                       |
|--------------------------------|---------------------------------------------------|
| **Standard KD**                | Hinton-style soft targets                        |
| **Cosine Similarity KD**       | Matches cosine of logits/embeddings              |
| **Intermediate Representation**| Uses hidden feature alignment between models     |
| **Relational KD (RKD)**        | Matches student and teacher relational distances |
| **VQA Distillation**           | Applied on DAQUAR VQA task (language + vision)   |

---

## ðŸ“ˆ Use Cases

- âœ… **ImageNet to CIFAR/Weather distillation**
- âœ… **Multimodal distillation (VQA)**
- âœ… **Teacher/Student baseline performance comparison**
- âœ… **Loss ablation: standard KD vs cosine vs intermediate**

---

## ðŸš€ Getting Started

### 1. Clone this repo
```bash
git clone https://github.com/melanieyes/knowledge-distillation.git
cd knowledge-distillation
